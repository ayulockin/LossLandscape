{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SmallCNN_Cifar10",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPHHkLcMTeLKNIgOOqMC+IW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayulockin/LossLandscape/blob/master/SmallCNN_Cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKzu9hbgqw2x",
        "colab_type": "text"
      },
      "source": [
        "# Setups, Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT_P8aSjO-ps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "b5b4f2e7-5d1f-4b97-9605-ec3eb3303dbe"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 14 19:49:48 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQAggd-btWJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e910c656-c009-4448-ea62-4cd744987091"
      },
      "source": [
        "## This is so that I can save my models.\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAZBTFNQq0ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-APP7-6C5uK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SEED = 666\n",
        "\n",
        "import tensorflow as tf\n",
        "# tf.random.set_seed(SEED)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications import resnet50\n",
        "\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pr3HrMxq8SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
        "\n",
        "import numpy as np\n",
        "# np.random.seed(SEED)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from PIL import Image"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVkQbw_q_07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f6613cce-fb2e-44aa-8c3b-42b63feb7b76"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://app.wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Ln8W0Aru8X",
        "colab_type": "text"
      },
      "source": [
        "# Download and Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMFYLweasySz",
        "colab_type": "text"
      },
      "source": [
        "#### CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Posjlu7lDJBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f6aeb040-dd5d-4bdf-8509-7a57461be677"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 9s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcM1SXdiGVf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASS_NAMES = (\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDLO83axDOzg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8e355e3c-c983-49c4-920d-054815204405"
      },
      "source": [
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZzY01xwxS-f",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TqcgM67xxDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 128\n",
        "IMG_SHAPE = 32\n",
        "\n",
        "trainloader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "testloader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "  img = tf.cast(image, tf.float32)\n",
        "  img = img/255.\n",
        "\n",
        "  return img, label\n",
        "\n",
        "trainloader = (\n",
        "    trainloader\n",
        "    .shuffle(1024)\n",
        "    .map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "testloader = (\n",
        "    testloader\n",
        "    .map(preprocess_image, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2xrs00QuVG3",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pv7-2VNsb5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Model():\n",
        "  inputs = keras.layers.Input(shape=(IMG_SHAPE, IMG_SHAPE, 3))\n",
        "\n",
        "  x = keras.layers.Conv2D(16, (3,3), padding='same')(inputs)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = keras.layers.MaxPooling2D(2, strides=2)(x)\n",
        "\n",
        "  x = keras.layers.Conv2D(32,(3,3), padding='same')(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = keras.layers.MaxPooling2D(2, strides=2)(x)\n",
        "\n",
        "  x = keras.layers.Conv2D(32,(3,3), padding='same')(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = keras.layers.MaxPooling2D(2, strides=2)(x)\n",
        "\n",
        "  x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = keras.layers.Dense(32, activation='relu')(x)\n",
        "  x = keras.layers.Dropout(0.1)(x)\n",
        "  \n",
        "  outputs = keras.layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "  return keras.models.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkxDAl8pv5pJ",
        "colab_type": "text"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4WY47Kxv8uN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom LR schedule as mentioned in the LossLandscape paper\n",
        "LR_SCHEDULE = [\n",
        "    # (epoch to start, learning rate) tuples\n",
        "    (0, 1.6*1e-3),\n",
        "    (9, (1.6*1e-3)/2),\n",
        "    (19, (1.6*1e-3)/4),\n",
        "    (29, (1.6*1e-3)/8),\n",
        "]\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    if (epoch >= 0) & (epoch < 9):\n",
        "        return LR_SCHEDULE[0][1]\n",
        "    elif (epoch >= 9) & (epoch < 19):\n",
        "        return LR_SCHEDULE[1][1]\n",
        "    elif (epoch >= 19) & (epoch < 29):\n",
        "        return LR_SCHEDULE[2][1]\n",
        "    else:\n",
        "        return LR_SCHEDULE[3][1]\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MocEmKKS5iNM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5bbf4fbf-54a9-4d10-ad27-7bb4cf726e96"
      },
      "source": [
        "%cd ../"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjb25zUM-2wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAVE_PATH = 'gdrive/My Drive/LossLandscape/SmallCNN_CheckpointID/'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48WXaSSx4hAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(epoch, logs):\n",
        "  model.save(SAVE_PATH+'small_cnn_checkpoint_{}.h5'.format(epoch))\n",
        "\n",
        "save_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=save_model, verbose=True)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q02l2PnW4YMh",
        "colab_type": "text"
      },
      "source": [
        "# Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7NGdxodMYP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "d37e5325-3b57-464f-ded0-3b8077a83f9e"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "model = Model()\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 32, 16)        448       \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu (TensorFlow [(None, 32, 32, 16)]      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 32)        4640      \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_1 (TensorFl [(None, 16, 16, 32)]      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 32)          9248      \n",
            "_________________________________________________________________\n",
            "tf_op_layer_Relu_2 (TensorFl [(None, 8, 8, 32)]        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 15,722\n",
            "Trainable params: 15,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqiY2v174cCw",
        "colab_type": "text"
      },
      "source": [
        "# Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7Vk7qmEMgeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP5elumu4eJG",
        "colab_type": "text"
      },
      "source": [
        "# Wandb Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svw-ocG_yeGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6aec0b81-5f28-4c47-a419-4ba890f7e89b"
      },
      "source": [
        "wandb.init(entity='ayush-thakur', project='loss-landscape')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/ayush-thakur/loss-landscape\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/loss-landscape</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/ayush-thakur/loss-landscape/runs/xujfmsju\" target=\"_blank\">https://app.wandb.ai/ayush-thakur/loss-landscape/runs/xujfmsju</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "W&B Run: https://app.wandb.ai/ayush-thakur/loss-landscape/runs/xujfmsju"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co0IcwRl4goi",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mqy_ZBNM8UA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eda1e8b7-2dcc-4b1b-cc32-900647783f76"
      },
      "source": [
        "EPOCHS = 40\n",
        "\n",
        "_ = model.fit(trainloader,\n",
        "          epochs=EPOCHS,\n",
        "          validation_data=testloader,\n",
        "          callbacks=[WandbCallback(),\n",
        "                     lr_callback,\n",
        "                     save_callback])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 1/40\n",
            "  1/391 [..............................] - ETA: 0s - loss: 2.2972 - acc: 0.1250WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.184638). Check your callbacks.\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.9067 - acc: 0.2726 - val_loss: 1.6830 - val_acc: 0.3711 - lr: 0.0016\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 2/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.6485 - acc: 0.3777 - val_loss: 1.6912 - val_acc: 0.3686 - lr: 0.0016\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 3/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.5356 - acc: 0.4272 - val_loss: 1.4627 - val_acc: 0.4642 - lr: 0.0016\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 4/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.4506 - acc: 0.4632 - val_loss: 1.3512 - val_acc: 0.5029 - lr: 0.0016\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 5/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.3763 - acc: 0.4903 - val_loss: 1.3305 - val_acc: 0.5033 - lr: 0.0016\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 6/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.3338 - acc: 0.5124 - val_loss: 1.2655 - val_acc: 0.5355 - lr: 0.0016\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 7/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.2880 - acc: 0.5287 - val_loss: 1.2682 - val_acc: 0.5390 - lr: 0.0016\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 8/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.2547 - acc: 0.5426 - val_loss: 1.1941 - val_acc: 0.5663 - lr: 0.0016\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0016.\n",
            "Epoch 9/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.2123 - acc: 0.5569 - val_loss: 1.1873 - val_acc: 0.5694 - lr: 0.0016\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 10/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.1671 - acc: 0.5782 - val_loss: 1.1250 - val_acc: 0.5920 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 11/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.1515 - acc: 0.5843 - val_loss: 1.1132 - val_acc: 0.5959 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 12/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.1360 - acc: 0.5899 - val_loss: 1.1211 - val_acc: 0.6009 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 13/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.1234 - acc: 0.5927 - val_loss: 1.1112 - val_acc: 0.6012 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 14/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.1062 - acc: 0.6012 - val_loss: 1.1003 - val_acc: 0.6037 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 15/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.1018 - acc: 0.6012 - val_loss: 1.0672 - val_acc: 0.6178 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 16/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0837 - acc: 0.6100 - val_loss: 1.0662 - val_acc: 0.6154 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 17/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0780 - acc: 0.6123 - val_loss: 1.0923 - val_acc: 0.6128 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 18/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0676 - acc: 0.6167 - val_loss: 1.0624 - val_acc: 0.6171 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0008.\n",
            "Epoch 19/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0537 - acc: 0.6225 - val_loss: 1.0632 - val_acc: 0.6194 - lr: 8.0000e-04\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 20/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0325 - acc: 0.6279 - val_loss: 1.0286 - val_acc: 0.6312 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 21/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0245 - acc: 0.6303 - val_loss: 1.0508 - val_acc: 0.6241 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 22/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0217 - acc: 0.6334 - val_loss: 1.0313 - val_acc: 0.6323 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 23/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0145 - acc: 0.6353 - val_loss: 1.0141 - val_acc: 0.6372 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 24/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0120 - acc: 0.6360 - val_loss: 1.0117 - val_acc: 0.6376 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 25/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0047 - acc: 0.6404 - val_loss: 1.0065 - val_acc: 0.6407 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 26/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0019 - acc: 0.6414 - val_loss: 1.0037 - val_acc: 0.6408 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 27/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 1.0007 - acc: 0.6402 - val_loss: 1.0010 - val_acc: 0.6448 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 28/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9902 - acc: 0.6439 - val_loss: 1.0007 - val_acc: 0.6424 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0004.\n",
            "Epoch 29/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9881 - acc: 0.6448 - val_loss: 1.0088 - val_acc: 0.6441 - lr: 4.0000e-04\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 30/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9770 - acc: 0.6488 - val_loss: 0.9976 - val_acc: 0.6451 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 31/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9743 - acc: 0.6498 - val_loss: 0.9925 - val_acc: 0.6460 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 32/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9688 - acc: 0.6520 - val_loss: 0.9871 - val_acc: 0.6502 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 33/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9676 - acc: 0.6541 - val_loss: 0.9866 - val_acc: 0.6509 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 34/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9646 - acc: 0.6543 - val_loss: 0.9884 - val_acc: 0.6477 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 35/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9640 - acc: 0.6558 - val_loss: 0.9818 - val_acc: 0.6526 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 36/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9650 - acc: 0.6538 - val_loss: 0.9871 - val_acc: 0.6489 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 37/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9615 - acc: 0.6558 - val_loss: 0.9881 - val_acc: 0.6481 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 38/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9601 - acc: 0.6553 - val_loss: 0.9856 - val_acc: 0.6495 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 39/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9572 - acc: 0.6578 - val_loss: 0.9862 - val_acc: 0.6478 - lr: 2.0000e-04\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 40/40\n",
            "391/391 [==============================] - 3s 8ms/step - loss: 0.9536 - acc: 0.6590 - val_loss: 0.9736 - val_acc: 0.6545 - lr: 2.0000e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKv6UHqM1I3Q",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO_E5WFAclHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c8ea9316-b59d-446c-8a59-9b69c45ec928"
      },
      "source": [
        "loss, accuracy = model.evaluate(testloader, callbacks=[WandbCallback()])\n",
        "print(\"Test Error Rate: \", round((1-accuracy)*100, 2), '%')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 0s 5ms/step - loss: 0.9736 - acc: 0.6545\n",
            "Test Error Rate:  34.55 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}